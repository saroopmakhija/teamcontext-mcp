from pydantic import BaseModel
from typing import List, Optional, Dict, Any
from datetime import datetime

# Legacy schemas - keeping for backward compatibility
class ContextSaveRequest(BaseModel):
    content: str
    project_id: Optional[str] = None
    source: str = "api"
    # tags are auto-generated by finding top 3 similar chunks

class ContextSearchRequest(BaseModel):
    query: str
    project_id: Optional[str] = None
    limit: int = 10
    similarity_threshold: float = 0.01

class ContextResponse(BaseModel):
    id: str
    content: str
    similarity_score: Optional[float] = None
    metadata: dict
    created_at: datetime

# New schemas for chunking workflow
class ChunkInput(BaseModel):
    """Input for chunking service - represents a single chunk of content"""
    content: str
    metadata: Optional[Dict[str, Any]] = {}

class ChunkAndEmbedRequest(BaseModel):
    """Request to chunk content, generate embeddings, and store in vector DB"""
    project_id: str  # REQUIRED: All vectors must be project-specific
    chunks: List[ChunkInput]  # List of pre-chunked content from chunking service
    source: str = "mcp_client"
    # tags are auto-generated by finding top 3 similar chunks

class VectorRetrievalRequest(BaseModel):
    """Request to retrieve similar vectors from project-specific vector DB"""
    project_id: str  # REQUIRED: Only search within this project
    query: str  # The query to search for
    limit: int = 10
    similarity_threshold: float = 0.01

class VectorRetrievalResponse(BaseModel):
    """Response with similar chunks from vector DB"""
    chunk_id: str
    content: str
    similarity_score: float
    metadata: Dict[str, Any]
    created_at: datetime

# RAG Chatbot schemas
class ChatMessage(BaseModel):
    """Single chat message"""
    role: str  # "user" or "model"
    content: str

class ChatRequest(BaseModel):
    """Request for RAG chatbot conversation"""
    project_id: str  # REQUIRED: Project-specific context
    message: str  # User's message
    history: Optional[List[ChatMessage]] = []  # Previous conversation history
    max_context_chunks: int = 5  # Number of relevant chunks to retrieve
    similarity_threshold: float = 0.3  # Lower threshold for broader context
    stream: bool = False  # Enable streaming response

class ChatResponse(BaseModel):
    """Response from RAG chatbot"""
    message: str  # AI response
    sources: List[VectorRetrievalResponse]  # Retrieved context chunks used
    conversation_id: Optional[str] = None  # For tracking conversations
